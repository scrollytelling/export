#!/usr/bin/env bash
set -euo pipefail
IFS=$'\n\t'

if [ $# -eq 0 ]
then
	echo "This script exports an entire Scrollytelling account to HTML."
	echo "The export is dumped in HOME for your enjoyment. Don't swallow everything at once."
	echo
	echo "Pass the hostname you wish to export as paramater."
	echo "Usage: $0 <cname>"
	echo "  e.g. $0 stories.example.com"
	echo
	exit 1
fi

set -vx

deps=( aws curl exiftool parallel http ruby )
for i in "${deps[@]}"
do
	if ! command -v $1; then
		echo "Couldn't find $1, which is required to proceed."
		echo
		exit 1
	fi
done

# Setup variables to use throughout the script.
cname=$1
dest="${HOME}/${cname}"
SCROLLY="${HOME}/Rails/scrollytelling"

# Bundler, for Ruby
gem install bundler --conservative
bundle check || bundle install

if [ ! -f "${dest}/index.json" ]
then
	# STEP: query the database and build index.json
	$SCROLLY/bin/rails runner ./step_1_build_index/published_entries.rb $cname
fi

# STEP: download live index.html to story folders.
bin/20_canonical_urls.rb $cname | parallel --jobs 5 \
	http --ignore-stdin --timeout 60 --download --output $dest/{/}/original.html GET {}

# and the story-specific CSS
bin/20_canonical_urls.rb $cname | parallel --jobs 5 \
	http --ignore-stdin --timeout 60 GET https://$cname/entries/{/}.css \
	"|"ruby ./step_2_download_story/replace.rb ">"$dest/entries/{/}.css

# STEP: massage the HTML until it's fit for archiving.
parallel --jobs 5 \
	ruby ./step_2_download_story/replace.rb \
	"<"{} ">"{//}/index.html ::: $dest/*/original.html

# STEP: sync media bucket
#   add --dryrun to see what it will do
./step_3_sync_assets/media_folders $cname | parallel --jobs 1 \
	"aws --profile scrollytelling s3 sync s3://{} ${dest}/{}"

# STEP: sync output bucket. Bucket name is different from local path.
#   add --dryrun to see what it will do
./step_3_sync_assets/output_folders $cname | parallel --jobs 1 \
  "aws --profile scrollytelling s3 sync s3://storyboard-pageflow-production-out/{} ${dest}/output.scrollytelling.com/{}"

# STEP: unpack static assets.
unzip -u ./artifacts/assets.zip -d $dest/scrollytelling.link
unzip -u ./artifacts/output.zip -d $dest/output.scrollytelling.com
unzip -u ./artifacts/root.zip -d $dest

# STEP: make browser screenshots for each page of every story.
./step_4_screenshots/grab $cname

# Convert the PNGs to JPGs
if [ -x "$(command -v vips)" ]
then
	parallel --jobs 10 vips jpegsave {} {.}.jpg ::: ${dest}/**/screenshots/*.png
	vipsthumbnail ${dest}/**/screenshots/*.png --format=%s_280.jpg --size=280
fi

# STEP: Generate checksums, to verify archive authenticity.
cd $dest
rm --force SHA512SUMS
find -name '*.html' -type f | parallel --jobs 10 "sha512sum --text {} >>SHA512SUMS"
find media.scrollytelling.com -path '*/original/*' -type f | parallel --jobs 10 "sha512sum --binary {} >>SHA512SUMS"
cd -

# STEP: compile templates based on exported information.
cd $HOME/archive
./deploy.sh $cname
cd -

# STEP: backdate timestamps on archived files
./step_4_ctimes/run.rb $cname

# STEP: export account-specific slice of the database
bin/70_database.sh $cname


## HOSTING

# Sync to our long-term archive
#rsync \
#  --compress \
#  --delete \
#  --human-readable \
#  --no-owner \
#  --partial-dir=.rsync \
#  --progress \
#  --recursive \
#  --safe-links \
#  --times \
#  --verbose \
#  $cname $remote_location
